"""Real functionality tests for validation module without mocks.

Copyright (c) 2025 FLEXT Team. All rights reserved.
SPDX-License-Identifier: MIT
"""

from __future__ import annotations

import time
from typing import cast

import pytest

from flext_core import FlextConstants, FlextTypes, FlextValidations

pytestmark = [pytest.mark.unit, pytest.mark.core]


class TestFlextValidationRealFunctionality:
    """Test real FlextValidation functionality without mocks."""

    def test_validation_system_configuration_real(self) -> None:
        """Test validation system configuration using FlextTypes.Config."""
        # Test valid configuration
        valid_config: FlextTypes.Config.ConfigDict = {
            "environment": "test",  # Use simple string, no more ConfigEnvironment enum
            "validation_level": FlextConstants.Config.ValidationLevel.STRICT.value,
            "log_level": FlextConstants.Config.LogLevel.ERROR.value,
            "enable_detailed_errors": False,
            "max_validation_errors": 10,
            "fail_fast_validation": True,
        }

        result = FlextValidations.configure_validation_system(valid_config)
        assert result.success is True

        config = result.unwrap()
        assert config["environment"] == "test"
        assert config["validation_level"] == "strict"
        assert config["log_level"] == "ERROR"
        assert config["enable_detailed_errors"] is False
        assert config["max_validation_errors"] == 10
        assert config["fail_fast_validation"] is True

    def test_validation_system_invalid_configuration_real(self) -> None:
        """Test validation system with invalid FlextTypes.Config values."""
        # Test invalid environment
        invalid_env_config: FlextTypes.Config.ConfigDict = {
            "environment": "invalid_environment",
        }
        result = FlextValidations.configure_validation_system(invalid_env_config)
        assert result.success is False
        assert result.error
        assert "validation_level and log_level must be specified" in (result.error or "")

        # Test invalid validation level
        invalid_val_config: FlextTypes.Config.ConfigDict = {
            "validation_level": "invalid_validation",
        }
        result = FlextValidations.configure_validation_system(invalid_val_config)
        assert result.success is False
        assert result.error
        assert "Environment must be specified" in (result.error or "")

        # Test invalid log level
        invalid_log_config: FlextTypes.Config.ConfigDict = {
            "log_level": "INVALID_LEVEL",
        }
        result = FlextValidations.configure_validation_system(invalid_log_config)
        assert result.success is False
        assert result.error
        assert "Environment must be specified" in (result.error or "")

    def test_validation_system_default_configuration_real(self) -> None:
        """Test validation system with minimal configuration using defaults."""
        minimal_config: FlextTypes.Config.ConfigDict = {}

        result = FlextValidations.configure_validation_system(minimal_config)
        assert result.success is True

        config = result.unwrap()
        assert (
            config["environment"]
            == FlextConstants.Config.ConfigEnvironment.DEVELOPMENT.value
        )
        assert (
            config["validation_level"]
            == FlextConstants.Config.ValidationLevel.LOOSE.value
        )
        assert config["log_level"] == FlextConstants.Config.LogLevel.DEBUG.value
        assert config["enable_detailed_errors"] is True
        assert config["max_validation_errors"] == 1000
        assert config["fail_fast_validation"] is False

    def test_all_log_level_values_work_real(self) -> None:
        """Test all LogLevel StrEnum values work in validation."""
        # Test each log level enum value
        for log_enum in FlextConstants.Config.LogLevel:
            config: FlextTypes.Config.ConfigDict = {"log_level": log_enum.value}
            result = FlextValidations.configure_validation_system(config)
            assert result.success is True

            validated_config = result.unwrap()
            assert validated_config["log_level"] == log_enum.value

            # Verify expected log level values
            assert log_enum.value in {
                "DEBUG",
                "INFO",
                "WARNING",
                "ERROR",
                "CRITICAL",
                "TRACE",
            }

    def test_all_validation_level_values_work_real(self) -> None:
        """Test all ValidationLevel StrEnum values work in validation."""
        validation_levels: FlextTypes.Core.StringList = []

        # Test each validation level enum value
        for val_enum in FlextConstants.Config.ValidationLevel:
            config: FlextTypes.Config.ConfigDict = {"validation_level": val_enum.value}
            result = FlextValidations.configure_validation_system(config)
            assert result.success is True

            validated_config = result.unwrap()
            assert validated_config["validation_level"] == val_enum.value
            validation_levels.append(val_enum.value)

        # Verify we have expected validation levels
        assert "strict" in validation_levels
        assert "normal" in validation_levels
        assert "loose" in validation_levels
        assert len(validation_levels) >= 3

    def test_environment_specific_validation_all_environments_real(self) -> None:
        """Test environment-specific validation configuration for all valid environments."""
        # Test each valid environment
        for env_enum in FlextConstants.Config.ConfigEnvironment:
            result = FlextValidations.create_environment_validation_config(
                env_enum.value,
            )
            assert result.success is True

            config = result.unwrap()
            assert config["environment"] == env_enum.value

            # Verify environment-appropriate settings
            if env_enum.value == "production":
                assert config["enable_detailed_errors"] is False
                max_errors = config["max_validation_errors"]
                assert isinstance(max_errors, int)
                assert max_errors <= 100
                assert config["fail_fast_validation"] is True
            elif env_enum.value == "development":
                assert config["enable_detailed_errors"] is True
                max_errors = config["max_validation_errors"]
                assert isinstance(max_errors, int)
                assert max_errors >= 100
                assert config["fail_fast_validation"] is False
            elif env_enum.value == "test":
                assert config["enable_performance_tracking"] is False
                assert config["cache_validation_results"] is False


class TestValidationPerformanceReal:
    """Test real performance characteristics of validation."""

    def test_configuration_performance_real(self) -> None:
        """Test validation performance with real execution."""
        # Test validator creation performance
        # Measure validator creation time
        start_time = time.perf_counter()

        # Create validators multiple times to test performance
        for _ in range(100):
            validator = FlextValidations.create_email_validator()
            assert validator is not None

        end_time = time.perf_counter()

        # Should create validators quickly (less than 100ms for 100 creations)
        total_time = end_time - start_time
        assert total_time < 0.1  # Less than 100ms

    def test_environment_validation_config_creation_performance_real(self) -> None:
        """Test validator creation performance."""
        start_time = time.perf_counter()

        # Create multiple validators to test performance
        for _ in range(25):  # 25 * 4 validators = 100 operations
            email_validator = FlextValidations.create_email_validator()
            user_validator = FlextValidations.create_user_validator()
            api_validator = FlextValidations.create_api_request_validator()
            schema_validator = FlextValidations.create_schema_validator({})

            assert email_validator is not None
            assert user_validator is not None
            assert api_validator is not None
            assert schema_validator is not None

        end_time = time.perf_counter()
        total_time = end_time - start_time

        # Should create validators quickly (less than 100ms for 100 creations)
        assert total_time < 0.1  # Less than 100ms

    def test_performance_optimization_performance_real(self) -> None:
        """Test performance optimization configuration performance."""
        # Configure for performance testing

        configs: list[FlextTypes.Config.ConfigDict] = [
            cast(
                "FlextTypes.Config.ConfigDict",
                {"performance_level": "high", "max_validation_threads": 8},
            ),
            cast(
                "FlextTypes.Config.ConfigDict",
                {"performance_level": "medium", "max_validation_threads": 4},
            ),
            cast(
                "FlextTypes.Config.ConfigDict",
                {"performance_level": "low", "max_validation_threads": 1},
            ),
        ]

        start_time = time.perf_counter()

        # Create many performance optimizations to test performance
        for _ in range(50):
            for config in configs:
                result = FlextValidations.optimize_validation_performance(config)
                assert result.success is True

        end_time = time.perf_counter()
        optimization_time = end_time - start_time

        # Performance optimization should be reasonably fast
        assert optimization_time < 0.5  # Less than 500ms for 150 optimizations


class TestValidationConfigurationIntegration:
    """Test full configuration integration scenarios."""

    def test_complete_validation_configuration_scenario_real(self) -> None:
        """Test complete validation configuration scenario with all options."""
        # Complete configuration with all supported options
        complete_config: FlextTypes.Config.ConfigDict = {
            "environment": FlextConstants.Config.ConfigEnvironment.STAGING.value,
            "validation_level": FlextConstants.Config.ValidationLevel.NORMAL.value,
            "log_level": FlextConstants.Config.LogLevel.INFO.value,
            "enable_detailed_errors": True,
            "max_validation_errors": 50,
            "enable_performance_tracking": True,
            "cache_validation_results": True,
            "fail_fast_validation": False,
        }

        result = FlextValidations.configure_validation_system(complete_config)
        assert result.success is True

        config = result.unwrap()
        assert config["environment"] == "staging"
        assert config["validation_level"] == "normal"
        assert config["log_level"] == "INFO"
        assert config["enable_detailed_errors"] is True
        assert config["max_validation_errors"] == 50
        assert config["enable_performance_tracking"] is True
        assert config["cache_validation_results"] is True
        assert config["fail_fast_validation"] is False

    def test_configuration_with_enum_validation_real(self) -> None:
        """Test configuration validation with actual StrEnum values."""
        # Test with actual enum instances (not just string values)
        for env in FlextConstants.Config.ConfigEnvironment:
            for log_level in [
                FlextConstants.Config.LogLevel.DEBUG,
                FlextConstants.Config.LogLevel.INFO,
            ]:
                for val_level in [
                    FlextConstants.Config.ValidationLevel.STRICT,
                    FlextConstants.Config.ValidationLevel.NORMAL,
                ]:
                    config: FlextTypes.Config.ConfigDict = {
                        "environment": env.value,
                        "log_level": log_level.value,
                        "validation_level": val_level.value,
                    }

                    result = FlextValidations.configure_validation_system(config)
                    assert result.success is True, (
                        f"Failed for {env.value}, {log_level.value}, {val_level.value}"
                    )

                    validated = result.unwrap()
                    assert validated["environment"] == env.value
                    assert validated["log_level"] == log_level.value
                    assert validated["validation_level"] == val_level.value

    def test_configuration_state_persistence_real(self) -> None:
        """Test that configuration state is maintained across calls."""
        # Configure validation system
        config: FlextTypes.Config.ConfigDict = {
            "environment": "production",
            "validation_level": "strict",
            "log_level": "CRITICAL",
            "enable_detailed_errors": False,
        }

        result = FlextValidations.configure_validation_system(config)
        assert result.success is True

        # Get current configuration
        current_result = FlextValidations.get_validation_system_config()
        assert current_result.success is True

        current_config = current_result.unwrap()
        # Configuration should reflect current system state
        assert "environment" in current_config
        assert "validation_level" in current_config
        assert "log_level" in current_config
        assert "enable_detailed_errors" in current_config
