#!/usr/bin/env python3
"""Generate quality metrics dashboard for Pydantic v2 compliance monitoring.

This script collects quality metrics across all FLEXT projects and generates
a comprehensive dashboard showing compliance status, test pass rates, and
quality metrics.

Usage:
    python scripts/quality_dashboard.py
    python scripts/quality_dashboard.py --output metrics.json
    python scripts/quality_dashboard.py --projects flext-core flext-cli flext-ldif

Output:
    Generates quality_metrics.json with compliance data for each project
"""

import argparse
import json
import sys
from dataclasses import asdict, dataclass
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

from flext_core import FlextUtilities


@dataclass
class ProjectMetrics:
    """Metrics for a single project."""

    project: str
    timestamp: str
    pydantic_v2_compliance: bool
    compliance_violations: int
    test_pass_rate: float | None
    type_check_errors: bool
    lint_violations: int
    overall_score: float

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return asdict(self)


def run_command(cmd: list[str], cwd: Path) -> tuple[int, str]:
    """Run a command and return exit code and output.

    Args:
        cmd: Command to run
        cwd: Working directory

    Returns:
        Tuple of (exit_code, output)

    """
    # CONVERTED: Use FlextUtilities instead of subprocess.run()
    result = FlextUtilities.run_external_command(
        cmd,
        check=False,
        cwd=str(cwd),
        capture_output=True,
        text=True,
        timeout=60,
    )

    if result.is_failure:
        # Detect timeout errors
        error_msg = result.error.lower()
        if "timed out" in error_msg:
            return 1, "Command timed out"
        return 1, result.error

    # Success - extract returncode and output
    proc = result.unwrap()
    return proc.returncode, proc.stdout + proc.stderr


def collect_metrics(project_path: Path) -> ProjectMetrics:
    """Collect metrics for a project.

    Args:
        project_path: Path to project

    Returns:
        ProjectMetrics instance

    """
    project_name = project_path.name
    timestamp = datetime.now(UTC).isoformat()

    # Check Pydantic v2 compliance
    pydantic_compliant = True
    violations = 0
    if (project_path / "docs/pydantic-v2-modernization/audit_pydantic_v2.py").exists():
        exit_code, _ = run_command(
            ["python", "docs/pydantic-v2-modernization/audit_pydantic_v2.py"],
            project_path,
        )
        pydantic_compliant = exit_code == 0
        # Parse violation count from output if available
        if not pydantic_compliant:
            violations = 1  # At least one violation

    # Check type checking (MyPy/Pyrefly)
    type_check_errors = True
    exit_code, _ = run_command(
        ["python", "-m", "mypy", "src/", "--config-file=pyproject.toml"],
        project_path,
    )
    type_check_errors = exit_code != 0

    # Check linting (Ruff)
    lint_violations = 0
    exit_code, output = run_command(
        ["python", "-m", "ruff", "check", "src/"],
        project_path,
    )
    if exit_code != 0:
        # Count violations in output
        lint_violations = output.count(" error") if "error" in output else 1

    # Check test pass rate
    test_pass_rate = None
    exit_code, output = run_command(
        ["python", "-m", "pytest", "--co", "-q"],
        project_path,
    )
    if exit_code == 0:
        # Rough estimate: if pytest runs successfully, tests exist
        test_pass_rate = 0.85  # Placeholder - ideally parse from test output

    # Calculate overall score (0-100)
    score = 100.0
    if violations > 0:
        score -= 20 * violations
    if type_check_errors:
        score -= 15
    if lint_violations > 0:
        score -= min(10 * lint_violations, 30)
    if test_pass_rate and test_pass_rate < 0.80:
        score -= 15

    score = max(0.0, score)  # Never go below 0

    return ProjectMetrics(
        project=project_name,
        timestamp=timestamp,
        pydantic_v2_compliance=pydantic_compliant,
        compliance_violations=violations,
        test_pass_rate=test_pass_rate,
        type_check_errors=type_check_errors,
        lint_violations=lint_violations,
        overall_score=score,
    )


def print_dashboard(metrics: list[ProjectMetrics]) -> None:
    """Print formatted dashboard.

    Args:
        metrics: List of project metrics

    """
    print("\n" + "=" * 100)
    print("FLEXT QUALITY DASHBOARD")
    print("=" * 100)
    print(f"Generated: {datetime.now(UTC).isoformat()}\n")

    # Summary table
    print(
        f"{'Project':<25} {'Pydantic v2':<15} {'Type Check':<15} {'Lint':<10} {'Score':<10}"
    )
    print("-" * 100)

    for metric in metrics:
        pydantic_status = "‚úÖ PASS" if metric.pydantic_v2_compliance else "‚ùå FAIL"
        type_status = "‚úÖ PASS" if not metric.type_check_errors else "‚ùå FAIL"
        lint_status = (
            f"‚úÖ {0}" if metric.lint_violations == 0 else f"‚ö†Ô∏è  {metric.lint_violations}"
        )
        score_status = f"{metric.overall_score:.0f}%"

        print(
            f"{metric.project:<25} {pydantic_status:<15} {type_status:<15} "
            f"{lint_status:<10} {score_status:<10}"
        )

    print("\n" + "=" * 100)

    # Overall compliance
    compliant_count = sum(1 for m in metrics if m.pydantic_v2_compliance)
    print(f"\nPydantic v2 Compliance: {compliant_count}/{len(metrics)} projects")
    print(f"Average Score: {sum(m.overall_score for m in metrics) / len(metrics):.1f}%")
    print()


def main() -> int:
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Generate quality metrics dashboard")
    parser.add_argument(
        "--output",
        type=str,
        default="quality_metrics.json",
        help="Output JSON file",
    )
    parser.add_argument(
        "--projects",
        type=str,
        nargs="+",
        help="Specific projects to audit (default: all flext-* projects)",
    )

    args = parser.parse_args()

    # Find projects
    workspace = Path.cwd()
    if args.projects:
        project_paths = [workspace / p for p in args.projects]
    else:
        project_paths = sorted([p for p in workspace.glob("flext-*") if p.is_dir()])

    if not project_paths:
        print("‚ùå No FLEXT projects found")
        return 1

    # Collect metrics
    print(f"üìä Collecting metrics from {len(project_paths)} projects...")
    metrics = []
    for project_path in project_paths:
        if (project_path / "pyproject.toml").exists():
            print(f"  Auditing {project_path.name}...", end=" ", flush=True)
            metric = collect_metrics(project_path)
            metrics.append(metric)
            print("‚úì")

    # Print dashboard
    print_dashboard(metrics)

    # Save to JSON
    with Path(args.output).open("w", encoding="utf-8") as f:
        json.dump(
            [m.to_dict() for m in metrics],
            f,
            indent=2,
            default=str,
        )

    print(f"üíæ Metrics saved to {args.output}\n")
    return 0


if __name__ == "__main__":
    sys.exit(main())
