import asyncio
from collections.abc import Callable as Callable, Generator
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Generic, TypedDict, TypeVar
from unittest.mock import AsyncMock, MagicMock, Mock

import pytest
from _pytest.fixtures import SubRequest as SubRequest

from flext_core import (
    FlextAggregateRoot,
    FlextBaseHandler,
    FlextCommands,
    FlextContainer,
    FlextEntity,
    FlextEntityStatus,
    FlextEvent,
    FlextResult,
    FlextValueObject,
)

T = TypeVar("T")
E = TypeVar("E", bound=FlextEntity)
V = TypeVar("V", bound=FlextValueObject)

def pytest_configure(config: pytest.Config) -> None: ...
def reset_environment(monkeypatch: pytest.MonkeyPatch) -> None: ...
@dataclass
class TestDataBuilder[T]:
    def with_field(self, name: str, value: object) -> TestDataBuilder[T]: ...
    def with_id(self, id_value: str) -> TestDataBuilder[T]: ...
    def with_status(self, status: FlextEntityStatus) -> TestDataBuilder[T]: ...
    def build(self) -> dict[str, object]: ...
    def build_as(self, cls: type[T]) -> T: ...

@pytest.fixture
def test_builder() -> type[TestDataBuilder[object]]: ...

class TestScenario(Enum):
    HAPPY_PATH = "happy_path"
    ERROR_CASE = "error_case"
    BOUNDARY = "boundary"
    EDGE_CASE = "edge_case"
    PERFORMANCE = "performance"

TInput = TypeVar("TInput", default=object)
TExpected = TypeVar("TExpected", default=object)

@dataclass
class TestCase(Generic[TInput, TExpected]):
    id: str
    description: str
    input_data: TInput
    expected_output: TExpected | None
    expected_error: str | None = ...
    scenario: TestScenario = ...
    marks: list[pytest.MarkDecorator] = ...

@pytest.fixture
def fixture_factory() -> Callable[[str, object], object]: ...
def parametrized_data(request: SubRequest) -> dict[str, object]: ...
@pytest.fixture
def sample_data() -> dict[str, object]: ...
@pytest.fixture
def hypothesis_strategies() -> dict[str, object]: ...
@pytest.fixture
def sample_metadata() -> dict[str, str | float | list[str] | None]: ...
@pytest.fixture
def error_context() -> dict[str, str | None]: ...
@pytest.fixture
def test_user_data() -> dict[str, str | int | bool | list[str] | None]: ...
@pytest.fixture
def mock_factory() -> Callable[[str], Mock]: ...
@pytest.fixture
def async_mock_factory() -> Callable[[str], AsyncMock]: ...
@pytest.fixture
def mock_external_service() -> Generator[MagicMock]: ...
@pytest.fixture
def entity_factory() -> Callable[[str, dict[str, object]], FlextEntity]: ...
@pytest.fixture
def value_object_factory() -> Callable[[dict[str, object]], FlextValueObject]: ...
@pytest.fixture
def aggregate_factory() -> Callable[[str], FlextAggregateRoot]: ...
@pytest.fixture
def clean_container() -> Generator[FlextContainer]: ...
@pytest.fixture
def configured_container(
    clean_container: FlextContainer, mock_external_service: MagicMock
) -> FlextContainer: ...
@pytest.fixture
def temp_directory() -> Generator[Path]: ...
@pytest.fixture
def isolation_context() -> Generator[dict[str, object]]: ...
@pytest.fixture
def time_machine() -> Callable[[datetime], None]: ...
def clean_logging_state() -> Generator[None]: ...

class PerformanceMetrics(TypedDict):
    result: object
    execution_time: float
    memory_used: int
    peak_memory: int

@pytest.fixture
def performance_monitor() -> Callable[[Callable[[], object]], PerformanceMetrics]: ...
@pytest.fixture
def benchmark_data() -> dict[str, object]: ...
@pytest.fixture
def performance_threshold() -> dict[str, float]: ...
@contextmanager
def assert_performance(
    max_time: float = 1.0, max_memory: int = 10000000
) -> Generator[None]: ...
@pytest.fixture
def snapshot_manager(tmp_path: Path) -> Callable[[str, object], None]: ...
@pytest.fixture
def event_loop() -> Generator[asyncio.AbstractEventLoop]: ...
@pytest.fixture
async def async_client() -> object: ...
@pytest.fixture
def validators() -> dict[str, Callable[[object], bool]]: ...
@pytest.fixture
def assert_helpers() -> object: ...
@pytest.fixture
def test_cases() -> list[TestCase[object]]: ...
def parametrize_test_cases(
    test_cases: list[TestCase[object]],
) -> Callable[[object], object]: ...
@pytest.fixture
def command_factory() -> Callable[[str, dict[str, object]], FlextCommands.Command]: ...
@pytest.fixture
def handler_factory() -> Callable[[Callable[[object], object]], FlextBaseHandler]: ...
@pytest.fixture
def event_factory() -> Callable[[str, dict[str, object]], FlextEvent]: ...
def integration_setup() -> None: ...
@pytest.fixture
def database_connection() -> Generator[MagicMock]: ...
def e2e_environment() -> dict[str, object]: ...
def pytest_collection_modifyitems(config: object, items: list[object]) -> None: ...

class AssertHelpers:
    @staticmethod
    def assert_result_success(result: FlextResult[object]) -> None: ...
    @staticmethod
    def assert_result_failure(result: FlextResult[object]) -> None: ...
    @staticmethod
    def assert_result_ok(result: FlextResult[object]) -> None: ...
    @staticmethod
    def assert_result_fail(result: FlextResult[object]) -> None: ...

class MockFactory:
    @staticmethod
    def create_mock_config(**kwargs: object) -> object: ...

class PerformanceMonitor:
    start_time: float | None
    end_time: float | None
    def __init__(self) -> None: ...
    def start(self) -> None: ...
    def stop(self) -> None: ...
    @property
    def duration(self) -> float: ...

class TestBuilder:
    data: dict[str, object]
    def __init__(self) -> None: ...
    def with_field(self, name: str, value: object) -> TestBuilder: ...
    def build(self) -> object: ...

def assert_function_performance(
    func: Callable[[], object], max_duration: float = 1.0
) -> object: ...
@pytest.fixture
def temp_json_file() -> Generator[str]: ...
@pytest.fixture
def validation_test_cases() -> list[dict[str, object]]: ...
